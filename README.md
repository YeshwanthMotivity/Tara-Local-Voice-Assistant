# ğŸ¤– TARA â€“ Memo-Local Voice Assistant

TARA is a fully offline, privacy-first voice assistant that listens, thinks, speaks, and remembers â€” without relying on cloud services. It captures your voice input, transcribes it locally using Whisper.cpp, processes it using a locally hosted LLaMA3 language model via Ollama, and speaks the response using Piper TTS.

---

## ğŸ“Œ Features

- ğŸ™ï¸ **Voice-to-Text (STT)** via Whisper.cpp (local)
- ğŸ¤– **Local LLM Inference** using LLaMA3:8B with Ollama API
- ğŸ§  **Memory-Persistent Conversations** (stored in `memory.json`)
- ğŸ”Š **Text-to-Speech (TTS)** using Piper (Docker-based with Wyoming protocol)
- ğŸŒ **Flask API** for web or programmatic interaction
- ğŸ” 100% **Offline Execution** (no external API calls)
- ğŸ§¹ **Voice-triggered memory clearing**: say _â€œclear memoryâ€_

---

## ğŸ› ï¸ Technologies Used

| Component            |        Tool / Framework             |
|----------------------|-------------------------------------|
| **Audio Recording**  | `sounddevice`, `wave` (Python)      |
| **Transcription**    | `whisper.cpp` + `ggml-base.en.bin`  |
| **LLM Inference**    | `LLaMA3` via `Ollama` (localhost API)|
| **Backend**          | `Flask`, `asyncio`, `json`          |
| **TTS**              | `Piper` (Docker) + Wyoming Protocol |
| **Memory Storage**   | Local `memory.json` file            |
| **API Testing**      | Postman, Browser                    |

---

## ğŸ§© Architecture Overview

![archtara1](https://github.com/user-attachments/assets/27b45a43-6838-40f1-9dd0-1ecdc252e5cf)

## WorkFlow

1. ğŸ§ **`record_audio.py`** captures user audio as `output.wav`
2. ğŸ§  **`whisper.cpp`** transcribes the audio â†’ `output.txt`
3. ğŸ§© **`assistant.py`** processes the text:
   - If "clear memory" â†’ clears `memory.json`
   - Else â†’ sends context + prompt to LLaMA3 via Ollama
4. ğŸ§  Response is saved in memory (`memory.json`)
5. ğŸ”Š Reply is spoken via Piper TTS and played back
6. ğŸŒ Optional: Interact via REST API

---

## ğŸŒ Flask API Endpoints

| Method | Route         | Description                                   |
|--------|---------------|---------------------------------------------- |
| POST   | `/transcribe` | Runs full pipeline (record â†’ respond â†’ speak) |
| POST   | `/ask`        | Accepts direct text input, responds & speaks  |
| GET    | `/memory`     | Returns recent conversation history           |

---

## ğŸ“¦ Setup Instructions

### âœ… Prerequisites
- Python 3.9+
- Docker installed
- `whisper.cpp` built from source
- LLaMA3 model running via Ollama
- Piper TTS model (e.g., `en_US-lessac-medium`) downloaded

### ğŸ”§ Setup Steps

**1. Clone the repo**
```bash
git clone https://github.com/your-username/tara-voice-assistant
cd tara-voice-assistant
```

**2. Install dependencies**
```
pip install -r requirements.txt
```

**3. Build whisper.cpp (Windows / CMake)**
```
Follow: https://github.com/ggerganov/whisper.cpp
```

**4. Download Whisper and Piper models**
```
Whisper: ggml-base.en.bin
Piper: en_US-lessac-medium.onnx + .onnx.json
```

**5. Run Piper TTS in Docker**
```
docker run -it --rm -v "$HOME/piper/voices:/voices" -p 10200:10200 rhasspy/piper --voice en/en_US-lessac-medium
```

**6. Start Ollama (LLaMA3 model)**
```
ollama run llama3:8b
```

**7. Start the Flask server**
```
python app.py
```
---
## ğŸ§ª Testing the Assistant

You can test it using:

- ğŸ¤ **Voice interaction** via [`POST /transcribe`]
- ğŸ’¬ **Text prompt** via [`POST /ask`]
- ğŸ§  **Memory inspection** via [`GET /memory`]

You can use **Postman**, **browser**, or any HTTP client to call the endpoints.

---

## ğŸ“ Project Structure

```bash
.
â”œâ”€â”€ app.py                # Flask backend
â”œâ”€â”€ assistant.py          # Core pipeline logic
â”œâ”€â”€ record_audio.py       # Voice recorder
â”œâ”€â”€ my_tts.py             # Piper TTS connection
â”œâ”€â”€ llm.py                # LLaMA3 response handler
â”œâ”€â”€ memory.json           # Stores assistant memory
â”œâ”€â”€ output.txt / .wav     # Temp transcription/output
â””â”€â”€ Docker & Model Setup  # External setup
```
---

## Use Cases

ğŸ” Secure offline voice assistant â€” no cloud dependency

ğŸ§  Privacy-focused AI environments â€” sensitive data never leaves device

ğŸ—£ï¸ Voice-controlled automation â€” trigger local commands or responses

ğŸ  Smart home or embedded voice apps â€” lightweight and efficient

ğŸ’¬ Voice journaling or mental health companion â€” with memory retention

ğŸŒ Offline assistant in remote areas â€” works without internet

---

## ğŸ™‹â€â™‚ï¸ Author

â€¢ Mentor / Manager: Mr. Venkata Ramana Sudhakar Polavarapu

â€¢ Mudimala Yeshwanth Goud

 ğŸ› ï¸ Passionate about AI/ML, NLP, RAG, Data Science, system programming, full-stack development, and intelligent assistant systems.


---
## ğŸ“¬ Contact
For questions or collaboration, you can reach out at:

**Email ğŸ“§** : yeshwanth.mudimala@motivitylabs.com
